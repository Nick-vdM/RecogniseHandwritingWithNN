"""
From https://mlfromscratch.com/neural-network-tutorial/#/
"""
import csv
import gzip
import io
from datetime import datetime
import numpy as np

xTrain = [[0.1, 0.1], [0.1, 0.2]]
yTrain = [[1, 0], [0, 1]]


class neuralNetwork:
    def __init__(self, layerSizes, epochs=1, batchSize=2, learningRate=0.1):
        self.epochs = epochs
        self.batchSize = batchSize
        self.learningRate = learningRate
        self.layerSizes = layerSizes

        # we save all parameters in the neural network in this dictionary
        self.parameters = self.initLayers()
        print("Completed initialisation")

    def sigmoid(self, value, derived=False):
        """
        The only activation function used in this model
        """
        if derived:
            return np.multiply(self.sigmoid(value),
                               np.subtract(1, self.sigmoid(value)))
        return np.divide(1, np.add(1, np.exp(np.negative(value))))

    def initLayers(self):
        # Just rename the layer sizes for reading
        inputLayer = self.layerSizes[0]
        hiddenLayer = self.layerSizes[1]
        outputLayer = self.layerSizes[2]

        parameters = {
            'HdnW': np.array([[0.1, 0.1], [0.2, 0.1]]),
            'HdnB': np.array([0.1, 0.1]),
            'OutW': np.array([[0.1, 0.1], [0.1, 0.2]]),
            'OutB': np.array([0.1, 0.1])
        }

        return parameters

    def forwardsPass(self, xTrain):
        """
        Does a forwards pass through the layers and updates the parameters
        :param xTrain: Input data to pass
        :return:
        """
        tempParameters = self.parameters
        # Label the first layer's output as the input
        tempParameters['out_input'] = np.array(xTrain)

        # Fold through first layer using initial input data
        tempParameters['net_h'] = np.add(
            np.dot(tempParameters['HdnW'], tempParameters['out_input']),
            tempParameters['HdnB']
        )
        tempParameters['out_h'] = self.sigmoid(tempParameters['net_h'])

        # The same operation as before, but we reuse the previous values
        tempParameters['net_o'] = np.add(
            np.dot(tempParameters['OutW'], tempParameters['out_h']),
            tempParameters['OutB']
        )

        tempParameters['out_o'] = self.sigmoid(tempParameters['net_o'])

        # print("===============================")
        # print("net_h=", tempParameters['net_h'])
        # print("out_h=", tempParameters['out_h'])
        # print("net_o=", tempParameters['net_o'])
        # print("out_o=", tempParameters['out_o'])

        return tempParameters['out_o']

    def backwards(self, yTrain, output):
        """
        Calculates the pdev error / weights
        :param yTrain: Expected output
        :param output: Generated by the model
        :return:
        """
        changeInWeights = {}

        # A simplification of the pdev error / pdev out_o1 is just output -
        # expected value
        error = output - yTrain
        pdevOuto_pdevNeto = \
            np.multiply(self.parameters['out_o'],
                        np.subtract(1, self.parameters['out_o']))
        changeInWeights['OutB'] = error * pdevOuto_pdevNeto
        changeInWeights['OutW'] = np.dot(
            changeInWeights['OutB'].reshape((len(changeInWeights['OutB']), 1)),
            self.parameters['out_h'].reshape(1, len(self.parameters['out_h']))
        )

        # Roll back to the next layer using the same idea
        error = np.multiply(np.dot(self.parameters['OutW'].transpose(), error),
                            self.sigmoid(self.parameters['net_h'], derived=True))
        pdevOuth_pdevNeth = \
            np.multiply(self.parameters['out_h'],
                        np.subtract(1, self.parameters['out_h']))
        changeInWeights['HdnB'] = np.multiply(error, pdevOuth_pdevNeth)

        changeInWeights['HdnW'] = np.dot(
            changeInWeights['HdnB'].reshape((len(changeInWeights['HdnB']), 1)),
            self.parameters['out_input'].reshape((1, len(self.parameters['out_input'])))
        )

        # print("////////////////////////////////")
        # print("Output layer change in weights", changeInWeights['OutW'])
        # print("Output layer change in biases", changeInWeights['OutB'])
        # print("Hidden Layer change in weights", changeInWeights['HdnW'])
        # print("Output layer change in biases", changeInWeights['HdnB'])

        return changeInWeights

    def updateWeights(self, changeInWeights):
        for key, value in changeInWeights.items():
            self.parameters[key] -= self.learningRate * value

    def printWeights(self):
        """
        Since the weights are stored in a pretty weird order we have to print
        them accordingly. This function is not kept in the full model and is
        just used to demonstrate what the data does up to 3 epochs
        :return:
        """
        print("\tw1:", self.parameters['HdnW'][0][0], "\t\tw2:", self.parameters['HdnW'][1][0])
        print("\tw3:", self.parameters['HdnW'][0][1], "\tw4:", self.parameters['HdnW'][1][1])

        print("\tw5:", self.parameters['OutW'][0][0], "\tw6:", self.parameters['OutW'][1][0])
        print("\tw7:", self.parameters['OutW'][0][1], "\tw8:", self.parameters['OutW'][1][1])

        print("\tw9:", self.parameters['HdnB'][0], "\tw10:", self.parameters['HdnB'][1])
        print("\tw11:", self.parameters['OutB'][0], "\tw12:", self.parameters['OutB'][1])

    def train(self, xTrain, yTrain):
        print("=================Initial Weights=================")
        self.printWeights()
        numberOfBatches = round(len(xTrain) / self.batchSize)
        # Split the data into batches
        trainSet = np.array(list(zip(xTrain, yTrain)))

        for epoch in range(self.epochs):
            # We actually want to randomise the groups of the batches
            # for variety in a full scale NN
            batches = [
                trainSet[x:np.add(x, self.batchSize)]
                for x in range(0, len(trainSet), self.batchSize)
            ]
            for batchNumber in range(numberOfBatches):
                # Since we're actually storing a lot more values in our parameters
                # we can't just do a for loop through this
                sumGradW = {}
                for key, value in self.parameters.items():
                    sumGradW[key] = np.zeros(value.shape)

                for x, y in batches[batchNumber]:
                    output = self.forwardsPass(x)
                    newValues = self.backwards(y, output)
                    for key, value in newValues.items():
                        sumGradW[key] += value

                for value in sumGradW.values():
                    value /= batches[batchNumber].shape[0]

                # Note: we averaged the sums
                self.updateWeights(sumGradW)
            print("================ Updated Weights: Epoch", epoch + 1, "================")
            self.printWeights()


if __name__ == "__main__":
    NN = neuralNetwork(layerSizes=[2, 2, 2], epochs=3, batchSize=2, learningRate=0.1)
    NN.train(xTrain, yTrain)
