"""
From https://mlfromscratch.com/neural-network-tutorial/#/
"""
import random

import numpy as np
import time
import matplotlib.pyplot as plt
import gzip
import csv
import os
import sys
from datetime import datetime


def openFile(fileName):
    return np.loadtxt(gzip.open(fileName, "rb"), delimiter=",")


# Just store these as global variables here so they don't have to be reloaded
NInput = 784
NHidden = 30
NOutput = 10
trainsetX_csv_gz = "data/TrainDigitX.csv.gz"
trainsetY_csv_gz = "data/TrainDigitY.csv.gz"
testsetX_csv_gz = "data/TestDigitX.csv.gz"
testSetY_csv_gz = "data/TestDigitY.csv.gz"
getXValid2 = True
if (len(sys.argv) == 7):
    NInput = sys.argv[0]
    NHidden = sys.argv[1]
    NOutput = sys.argv[2]
    trainsetX_csv_gz = sys.argv[3]
    trainsetY_csv_gz = sys.argv[4]
    testsetX_csv_gz = sys.argv[5]
    testSetY_csv_gz = sys.argv[6]
    getXValid2 = False  # it wasn't requested soooooooooo
else:
    print("None or the incorrect number of commandline arguments were passed. "
          "Default values are getting used")

print("Started loading in data")
start = time.perf_counter()
xTrain = openFile(trainsetX_csv_gz)
yTrain = openFile(trainsetY_csv_gz)
xValid1 = openFile(testsetX_csv_gz)
yValid1 = openFile(testSetY_csv_gz)
xValid2 = 0
if getXValid2:
    xValid2 = openFile("data/TestDigitX2.csv.gz")

print("Finished loading in the data! And it only took", time.perf_counter() - start,
      "seconds!")


def genLabelledVector(index):
    # Generates a numpy array NOutput long and sets the index to 1 while the
    # rest to 0
    a = np.zeros(NOutput)
    a[int(index)] = 1
    return a

class neuralNetwork:
    def __init__(self, layerSizes, epochs=30, batchSize=20, learningRate=3.0):
        self.epochs = epochs
        self.learningRate = learningRate
        self.layerSizes = layerSizes
        self.batchSize = batchSize

        # we save all parameters in the neural network in this dictionary
        self.parameters = self.initLayers()
        print("Completed initialisation")

    def sigmoid(self, value, derived=False):
        """
        The only activation function used in this model
        """
        if derived:
            return np.multiply(self.sigmoid(value),
                               np.subtract(1, self.sigmoid(value)))
        return np.divide(1, np.add(1, np.exp(np.negative(value))))

    def initLayers(self):
        # Just rename the layer sizes for reading
        inputLayer = self.layerSizes[0]
        hiddenLayer = self.layerSizes[1]
        outputLayer = self.layerSizes[2]

        parameters = {
            # In->Hidden weights
            'HdnW': np.random.normal(loc=0, scale=1, size=(hiddenLayer, inputLayer)),
            # Hidden biases
            'HdnB': np.random.normal(loc=0, scale=1, size=hiddenLayer),
            # Hidden->Output Weights
            'OutW': np.random.normal(loc=0, scale=1, size=(outputLayer, hiddenLayer)),
            # Output biases
            'OutB': np.random.normal(loc=0, scale=1, size=outputLayer)
        }

        return parameters

    def forwardsPass(self, xData):
        """
        Does a forwards pass through the layers and updates the parameters
        :param xData: Input data to pass
        :return:
        """
        # Label the first layer's output as the input
        self.parameters['out_input'] = np.array(xData)

        # Fold through first layer using initial input data
        self.parameters['net_h'] = np.add(
            np.dot(self.parameters['HdnW'], self.parameters['out_input']),
            self.parameters['HdnB']
        )
        self.parameters['out_h'] = self.sigmoid(self.parameters['net_h'])

        # The same operation as before, but we reuse the previous values
        self.parameters['net_o'] = np.add(
            np.dot(self.parameters['OutW'], self.parameters['out_h']),
            self.parameters['OutB']
        )

        self.parameters['out_o'] = self.sigmoid(self.parameters['net_o'])

        return self.parameters['out_o']

    def backwards(self, expectedOutput, modelOutput):
        """
        Calculates the pdev error / weights
        :param expectedOutput: Expected output
        :param modelOutput: Generated by the model
        :return:
        """
        changeInWeights = {}

        # A simplification of pdev error / pedv out_o1 is just output - expected
        # value. We'll just call this error for now
        error = modelOutput - genLabelledVector(expectedOutput)
        pdevOuto_pdevNeto = \
            np.multiply(self.parameters['out_o'],
                        np.subtract(1, self.parameters['out_o']))
        # Since our bias input is always 1 we can just set our biases to this
        changeInWeights['OutB'] = error * pdevOuto_pdevNeto
        changeInWeights['OutW'] = np.dot(
            changeInWeights['OutB'].reshape((len(changeInWeights['OutB']), 1)),
            self.parameters['out_h'].reshape(1, len(self.parameters['out_h']))
        )

        # Roll back to the next layer using the same idea
        error = np.multiply(np.dot(self.parameters['OutW'].transpose(), error),
                            self.sigmoid(self.parameters['net_h'], derived=True))
        pdevOuth_pdevNeth = \
            np.multiply(self.parameters['out_h'],
                        np.subtract(1, self.parameters['out_h']))
        changeInWeights['HdnB'] = np.multiply(error, pdevOuth_pdevNeth)

        changeInWeights['HdnW'] = np.dot(
            changeInWeights['HdnB'].reshape((len(changeInWeights['HdnB']), 1)),
            self.parameters['out_input'].reshape(1, (len(self.parameters['out_input'])))
        )

        return changeInWeights

    def updateWeights(self, changeInWeights):
        for key, value in changeInWeights.items():
            self.parameters[key] -= self.learningRate * value

    def rateModel(self, xValues, yValues):
        """
        Forwards pass through and compare the model's output to what it should be
        :param xValues:
        :param yValues:
        :return:
        """
        predictions = []

        for x, y in zip(xValues, yValues):
            output = self.forwardsPass(x)
            prediction = np.argmax(output)
            predictions.append(y == prediction)

        percentageValid = np.divide(sum(p for p in predictions), yValues.shape[0])
        percentageValid = np.multiply(percentageValid, 100)
        return percentageValid

    def train(self, xTrain, yTrain, xTest, yTest):
        accuracyHistory = [[], []]
        numberOfBatches = round(len(xTrain) / self.batchSize)
        startTime = time.perf_counter()
        # Split the data into batches
        trainSet = np.array(list(zip(xTrain, yTrain)))
        accuracy = self.rateModel(xTest, yTest)
        print("initial accuracy", accuracy)
        for epoch in range(self.epochs):
            # We actually want to randomise the groups of the batches
            # for variety in a full scale NN
            # random.shuffle(trainSet)
            batches = [
                trainSet[x:np.add(x, self.batchSize)]
                for x in range(0, len(trainSet), self.batchSize)
            ]
            for batchNumber in range(numberOfBatches):
                # Since we're actually storing a lot more values in our parameters
                # we can't just do a for loop through this
                sumGradW = {
                    'HdnW': np.zeros(self.parameters['HdnW'].shape),
                    'OutW': np.zeros(self.parameters['OutW'].shape),
                    'HdnB': np.zeros(self.parameters['HdnB'].shape),
                    'OutB': np.zeros(self.parameters['OutB'].shape)
                }

                for x, y in batches[batchNumber]:
                    output = self.forwardsPass(x)
                    newValues = self.backwards(y, output)
                    for key, value in newValues.items():
                        sumGradW[key] += value

                for value in sumGradW.values():
                    value /= batches[batchNumber].shape[0]

                # Note: we averaged the sums
                self.updateWeights(sumGradW)

            accuracy = self.rateModel(xTest, yTest)
            print("Epoch:", epoch + 1,
                  "time:", time.perf_counter() - startTime,
                  "percentage valid:", accuracy)
            startTime = time.perf_counter()
            accuracyHistory[0].append(epoch)
            accuracyHistory[1].append(accuracy)
        return accuracyHistory

    def saveResults(self, data=xValid1, filename=""):
        """
        Writes the forward pass output to a file
        :param filename:
        :return:
        """
        accuracy = self.rateModel(xValid1, yValid1)
        if filename == "":
            # Set the filename to results_datetime
            current = datetime.now()
            filename = "outputData/" + current.strftime("%Y-%m-%d_%H-%M-%S")
            filename += 'acc'+str(accuracy)
            filename += ".csv.gz"

        with gzip.open(filename, "wt", newline="") as file:
            csvW = csv.writer(file)
            for x in data:
                output = self.forwardsPass(x)
                prediction = np.argmax(output)
                csvW.writerow([prediction])


def generateGraph(data, labels, title="Accuracy"):
    """

    :param data:
    :param labels:
    :param title:
    :return:
    """
    print(data)
    print(labels)
    for point, label in zip(data, labels):
        XData = point[0]
        YData = point[1]

        plt.plot(XData, YData, label=label)
    plt.xlabel("Epoch number")
    plt.ylabel("Accuracy % (100 = all correct)")
    plt.title(title)
    plt.legend()
    # plt.show()

    fileName = ""
    # if this directory exists, put it there. otherwise just dump it in the sample
    # place as the file
    if os.path.exists(os.path.dirname(os.path.realpath(__file__)) + "/outputGraphs"):
        fileName += "outputGraphs/"
    fileName += datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    fileName += ".pdf"
    plt.savefig(fileName)

    # close the graph
    plt.clf()
    plt.cla()
    plt.close()


def differentLearning(batchSize=20, epochs=30, layerSizes=[NInput, NHidden, NOutput],
                      learningRates = [0.001, 0.1, 1.0, 10.0, 100.0]):
    """
    Does step 3 with learning rates 0.0001, 0.1, 1.0, 10, 100
    :return:
    """
    accuracyHistory = []
    labels = []
    for i in learningRates:
        NN = neuralNetwork(layerSizes=layerSizes, learningRate=i, epochs=epochs, batchSize=batchSize)
        accuracyHistory.append(NN.train(xTrain, yTrain, xValid1, yValid1))
        labels.append(str("Learning:" + str(NN.learningRate)))
    generateGraph(accuracyHistory, labels, title="Differing learning rates")


def differentBatchSize(epochs=30, layerSizes=[NInput, NHidden, NOutput], learningRate=3):
    """
    Generates all of the requested neural networks and graphs everything
    :return:
    """
    batchSizes = [1, 5, 10, 20, 100]
    accuracyHistory = []
    labels = []
    for i in batchSizes:
        NN = neuralNetwork(layerSizes=layerSizes, learningRate=learningRate,
                           epochs=epochs, batchSize=i)
        accuracyHistory.append(NN.train(xTrain, yTrain, xValid1, yValid1))
        labels.append(str("Batch Size:" + str(NN.batchSize)))
    generateGraph(accuracyHistory, labels, title="Differing batch sizes")


if __name__ == "__main__":
    start = time.perf_counter()
    NN = neuralNetwork(layerSizes=[NInput, NHidden, NOutput], learningRate=0.5, epochs=200, batchSize=15)
    accuracyHistory = [NN.train(xTrain, yTrain, xValid1, yValid1)]
    labels = ["Learning:" + str(NN.learningRate) + "Batch Size:" + str(NN.batchSize)]
    NN.saveResults(xValid1)
    if getXValid2:
        NN.saveResults(xValid2)
    generateGraph(accuracyHistory, labels)

    differentLearning(epochs=30, learningRates=[1, 5, 10, 15, 20, 40])
    # differentLearning(epochs=30)
    # differentBatchSize(epochs=30)
